\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{mahadevan1992automatic}
\citation{mahadevan1992automatic}
\citation{smart2002effective}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{1}}
\citation{liu2014review}
\citation{ozaslaninspection}
\citation{das2015devices}
\citation{kober2012reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation and applications to general scenarios}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Quadrotor navigating through a confined space without GPS signal. Images collected during field trial reported in chapter \ref  {bridge_chapter}}}{2}}
\newlabel{fig:motivation}{{1.1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Challenges}{2}}
\citation{chowdhary2014off}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Organization of the Thesis}{4}}
\citation{littman1996algorithms}
\citation{wiering2012reinforcement}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{background}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sequential Decision Making}{5}}
\newlabel{sequential_decision_making}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Approaches to solve sequential decision making problems}{5}}
\citation{puterman2014markov}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Rewards, and how to assign them?}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Markov Decision Processes}{6}}
\newlabel{mdp}{{2.2}{6}}
\newlabel{mdp_def}{{1}{6}}
\newlabel{transition_function}{{2.2}{6}}
\newlabel{reward_function}{{2.2}{6}}
\citation{Bellman:1957}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Policies}{7}}
\newlabel{policies}{{2.2.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Value function and Bellman equation}{7}}
\newlabel{value_function_equation}{{2.1}{7}}
\newlabel{value_function_state_action_equation}{{2.2}{7}}
\newlabel{bellman_equation}{{2.3}{7}}
\citation{kaelbling1998planning}
\citation{Bellman:1957}
\citation{bou2010controller}
\citation{lupashin2010simple}
\newlabel{bellman_optimality_equation}{{2.5}{8}}
\newlabel{v_q_relation}{{2.6}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Solving MDPs}{8}}
\newlabel{bellman}{{2.2.3}{8}}
\citation{mahadevan1992automatic}
\citation{bagnell2001autonomous}
\citation{kormushev2010robot}
\citation{Shaker:2010:VLS:1901614.1902128}
\citation{sugimoto2016acquisition}
\citation{kaelbling1996reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning (RL)}{9}}
\newlabel{rl}{{2.3}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A Simplified Reinforcement Learning Model}}{9}}
\newlabel{fig:rl_model}{{2.1}{9}}
\citation{watkins1992q}
\citation{rummery1994line}
\citation{hester2017learning}
\citation{brafman2002r}
\citation{kearns2002near}
\citation{strehl2006pac}
\citation{atkeson1997comparison}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Classification/Types}{10}}
\newlabel{types}{{2.3.1}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Model-free}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Model-based}{10}}
\citation{berry1985bandit}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Online vs off-line learning}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Exploration vs. Exploitation}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Q-Learning}{12}}
\newlabel{q_learning_section}{{2.3.4}{12}}
\newlabel{q_function}{{2.8}{12}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Q-Learning Algorithm}}{12}}
\newlabel{q_learning}{{1}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Example}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An example gridlworld with stochastic actions}}{13}}
\newlabel{fig:gridworld_ex}{{2.2}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Navigation with perception in the loop}}{14}}
\newlabel{fig:gp_laser}{{2.3}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Introduction to Gaussian Processes (GP)}{14}}
\newlabel{introToGPs}{{2.4}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces To find a function that is consistent over the observed data}}{14}}
\newlabel{fig:least_square}{{2.4}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Kernel Functions}{15}}
\citation{rasmussen2003gaussian}
\citation{rasmussen2003gaussian}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Gaussian Process Regression}{16}}
\newlabel{GPR}{{2.4.1}{16}}
\newlabel{equ:f_f*}{{2.13}{16}}
\citation{gamedeep}
\citation{morton2016deep}
\citation{mnih2013playing}
\citation{krizhevsky2012imagenet}
\citation{mnih2015human}
\citation{mnih2013playing}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Deep Reinforcement Learning}{17}}
\newlabel{deep_rl_section}{{2.5}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A naive representation of deep Q-network}}{17}}
\newlabel{fig:deep_q}{{2.5}{17}}
\citation{tokekar2014placement}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}The End-to-End GPQ Algorithm}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{gpq}{{3}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Perception in the loop}{19}}
\citation{koenig2004design}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A UAV equipped with a laser sensor}}{20}}
\newlabel{fig:perception_example}{{3.1}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Simulator setup}{20}}
\newlabel{sims}{{3.2}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A python based simulator for obstacle avoidance}}{20}}
\newlabel{fig:perception_sim_1}{{3.2}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A UAV equipped with laser sensor in Gazebo Robot Simulator}}{21}}
\newlabel{fig:perception_sim_2}{{3.3}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Laser data and $Q$-Learning}{21}}
\newlabel{laser_learning}{{3.2.1}{21}}
\citation{chowdhary2014off}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}End-to-End GPQ Algorithm}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Simulations and Results}{22}}
\newlabel{batch_gpq_algorithm}{{2}{23}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Batch GPQ Algorithm}}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Simulator Environments}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Environment 1}}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Environment 2}}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Environment 3}}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Environment 4}}}{24}}
\newlabel{fig:gpq_sim}{{3.4}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Reward as a function of time}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Reward obtained over time for environments shown in Figure \ref  {fig:gpq_sim}. One \textit  {epoch} is a set of $200$ actions.}}{25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Reward collected in Environment 1}}}{25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Reward collected in Environment 2}}}{25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Reward collected in Environment 3}}}{25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Reward collected in Environment 4}}}{25}}
\newlabel{fig:gpq_vs_q_reward}{{3.5}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Gazebo Robot Simulator environments and the reward collected over time. One \textit  {epoch} is a set of $20$ actions.}}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Gazebo environment 1}}}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Reward collected in Gazebo environment 1}}}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Gazebo environment 2}}}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Reward collected in Gazebo environment 2}}}{26}}
\newlabel{fig:gazebo_gpq_sim}{{3.6}{26}}
\citation{cutler2014reinforcement}
\citation{taylor2007transfer}
\citation{li2011knows}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}The GP-MFRL Algorithm}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{gpMfrl}{{4}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Multi-Fidelity Reinforcement Learning}{27}}
\citation{Dames2015}
\citation{rasmussen2003gaussian}
\citation{deisenroth2010efficient}
\citation{engel2005reinforcement}
\citation{puterman2014markov}
\citation{brafman2002r,kearns2002near}
\citation{sutton1998reinforcement}
\citation{strehl2006pac}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces MFRL framework: First simulator captures only gridworld movements of a point robot while second simulator has more fidelity using a physics simulator. Control can switch back and forth between simulators and real environment which is essentially the third simulator in the multi-fidelity simulator chain.}}{28}}
\newlabel{fig:mfrl_architecture}{{4.1}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Learning Transition Dynamics as a GP}{28}}
\citation{rasmussen2003gaussian}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Overview of the GP-MFRL algorithm }}{29}}
\newlabel{fig:gp_mfrl_system}{{4.2}{29}}
\newlabel{gaussian_mean}{{4.1}{29}}
\newlabel{gaussian_variance}{{4.2}{29}}
\citation{abbeel2006using,taylor2007transfer}
\citation{cutler2014reinforcement}
\citation{sutton1998reinforcement}
\newlabel{kernel_def}{{4.3}{30}}
\newlabel{velocity_def}{{4.4}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}GP-MFRL Algorithm}{30}}
\citation{koenig2004design}
\citation{quigley2009ros}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Simulation Results}{32}}
\newlabel{gp-mfrl-sim}{{4.4}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The environment setup for a multi-fidelity simulator chain. The simple gridworld environment has two wall obstacles whereas the gazebo environment has four wall obstacles as shown.}}{32}}
\newlabel{fig:gp_mfrl_setup}{{4.3}{32}}
\newlabel{lt}{{4}{33}}
\newlabel{forins}{{8}{33}}
\newlabel{forins1}{{17}{33}}
\newlabel{forins2}{{20}{33}}
\newlabel{forins3}{{22}{33}}
\newlabel{forins4}{{30}{33}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces GP-MFRL Algorithm}}{33}}
\newlabel{GP-MFRL}{{3}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces  The figure represents the samples collected in each level of simulator for a $21 \times 21$ grid in a simple grid-world and Gazebo environments. $\Psi $ and $\psi $ were kept 0.4 and $0.1$}}{34}}
\newlabel{fig:epoch_samples}{{4.4}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after transition dynamics initialization and after algorithm has converged}}{34}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {After Initial Training}}}{34}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After Convergence}}}{34}}
\newlabel{fig:heatmap1}{{4.5}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Representative simulations}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after transition dynamics initialization and after algorithm has converged}}{35}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {After Initial Training}}}{35}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After Convergence}}}{35}}
\newlabel{fig:heatmap2}{{4.6}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after the algorithm has converged. Walls A and B are only present in the grid-world simulator, whereas all four walls are present in the Gazebo simulator.}}{35}}
\newlabel{fig:heatmap_four_walls_complex}{{4.7}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Effect of fidelity on the number of samples.}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces As we make first simulator more inaccurate by adding noise, the agent tends to gather more samples in second simulator }}{36}}
\newlabel{fig:gp_mfrl_samples}{{4.8}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Ratio of samples gathered in the second simulator to the total samples gathered increases with inaccuracy in the first simulator. The reference line depicts the average number of samples gathered over $10$ runs when only Gazebo simulator was present.}}{36}}
\newlabel{fig:gp_mfrl_ratio}{{4.9}{36}}
\citation{cutler2014reinforcement}
\citation{cutler2014reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Effect of the confidence parameters.}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Ratio of samples gathered in second simulator vs. total samples gathered as we change the threshold or confidence parameters of the two simulators.}}{37}}
\newlabel{fig:threshold}{{4.10}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Comparison with R-max MFRL}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces  Discounted return in the start state Vs. the number of samples collected in the highest fidelity simulator.}}{38}}
\newlabel{fig:value_function}{{4.11}{38}}
\citation{morgenthal2014quality}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Bridge Inspection}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{bridge_chapter}{{5}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Background}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Conventional Inspection of Bridges}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Conventional inspection units for Bridge Inspection: (left,Bridge Inspection platform with a truck crane) , A-30 Hi-Rail Under Bridge Units(right,N.E. Bridge Contractors Inc.)}}{40}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Truck Crane}}}{40}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Under Bridge Inspection unit}}}{40}}
\newlabel{fig:bridge_inspection}{{5.1}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Bridgeriggers' Aspen A-75 under-bridge inspection crane overturns on Sakonnet River Bridge in Rhode Island; August 30, 2016}}{40}}
\newlabel{fig:bridge_fail}{{5.2}{40}}
\citation{djiframe}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}UAVs in Bridge Inspection}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Present Work}{41}}
\citation{fleacamera}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}The System Setup}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces UAV used for the experiments}}{42}}
\newlabel{fig:dji_frame}{{5.3}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces System Components}}{42}}
\newlabel{fig:system_components}{{5.4}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Flea3 2.0 MP Color USB3 Vision (e2v EV76C5706F)}}{43}}
\newlabel{fig:cam}{{5.5}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Intel NUC NUC5I7RYH}}{43}}
\newlabel{fig:nuc}{{5.6}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Pixhawk Flight Controller}}{44}}
\newlabel{fig:pixhawk}{{5.7}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces LIDAR-Lite v$3$}}{44}}
\newlabel{fig:lidar}{{5.8}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Ublox Neo-M$8$N GPS}}{45}}
\newlabel{fig:gps}{{5.9}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces E800 Tuned Propulsion System by DJI}}{46}}
\newlabel{fig:dji_propulsion_system}{{5.10}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Camera Software and ROS nodes}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Camera view and ROS-bag record}}{46}}
\newlabel{fig:cam_rqt}{{5.11}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Indoor flight of the UAV visually inspecting the structure}}{47}}
\newlabel{fig:indoor_flight}{{5.12}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Outdoor flight of the UAV visually inspecting the structure}}{47}}
\newlabel{fig:outdoor_flight}{{5.13}{47}}
\citation{osborne2010bayesian}
\citation{engel2005reinforcement}
\citation{csato2002sparse}
\bibdata{refs}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and Future Research}{48}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{djiframe}{1}
\bibcite{fleacamera}{2}
\bibcite{abbeel2006using}{3}
\bibcite{atkeson1997comparison}{4}
\bibcite{bagnell2001autonomous}{5}
\bibcite{Bellman:1957}{6}
\bibcite{berry1985bandit}{7}
\bibcite{bou2010controller}{8}
\bibcite{brafman2002r}{9}
\bibcite{chowdhary2014off}{10}
\bibcite{csato2002sparse}{11}
\bibcite{cutler2014reinforcement}{12}
\bibcite{Dames2015}{13}
\bibcite{das2015devices}{14}
\bibcite{deisenroth2010efficient}{15}
\bibcite{engel2005reinforcement}{16}
\bibcite{gamedeep}{17}
\bibcite{hester2017learning}{18}
\bibcite{kaelbling1998planning}{19}
\bibcite{kaelbling1996reinforcement}{20}
\bibcite{kearns2002near}{21}
\bibcite{kober2012reinforcement}{22}
\bibcite{koenig2004design}{23}
\bibcite{kormushev2010robot}{24}
\bibcite{krizhevsky2012imagenet}{25}
\bibcite{li2011knows}{26}
\bibcite{littman1996algorithms}{27}
\bibcite{liu2014review}{28}
\bibcite{lupashin2010simple}{29}
\bibcite{mahadevan1992automatic}{30}
\bibcite{mnih2013playing}{31}
\bibcite{mnih2015human}{32}
\bibcite{morgenthal2014quality}{33}
\bibcite{morton2016deep}{34}
\bibcite{osborne2010bayesian}{35}
\bibcite{ozaslaninspection}{36}
\bibcite{scikit-learn}{37}
\bibcite{puterman2014markov}{38}
\bibcite{quigley2009ros}{39}
\bibcite{rasmussen2003gaussian}{40}
\bibcite{rummery1994line}{41}
\bibcite{Shaker:2010:VLS:1901614.1902128}{42}
\bibcite{smart2002effective}{43}
\bibcite{strehl2006pac}{44}
\bibcite{sugimoto2016acquisition}{45}
\bibcite{sutton1998reinforcement}{46}
\bibcite{taylor2007transfer}{47}
\bibcite{tokekar2014placement}{48}
\bibcite{watkins1992q}{49}
\bibcite{wiering2012reinforcement}{50}
\bibstyle{plain}
