\relax 
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{main.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Applications to general scenarios and Challenges}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Subsection 1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Subsection 2}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Organization of the Thesis}{1}}
\citation{littman1996algorithms}
\citation{wiering2012reinforcement}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{background}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sequential Decision Making}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Approaches to solve sequential decision making}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Why Learning?}{3}}
\newlabel{whyLearning}{{2.1.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Online Vs Off-line Learning}{3}}
\citation{puterman2014markov}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Rewards, and how to assign them ?}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Markov Decision Processes}{4}}
\newlabel{mdp}{{2.2}{4}}
\newlabel{mdp_def}{{1}{4}}
\newlabel{transition_function}{{2.2}{4}}
\citation{kaelbling1996reinforcement}
\newlabel{reward_function}{{2.2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Policies}{5}}
\newlabel{policies}{{2.2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Solving MDPs}{5}}
\newlabel{bellman}{{2.2.2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning (RL)}{5}}
\newlabel{rl}{{2.3}{5}}
\citation{berry1985bandit}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Model of Reinforcement Learning}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A Simplified Reinforcement Learning Model}}{6}}
\newlabel{fig:rl_model}{{2.1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Exploration Vs. Exploitation}{6}}
\citation{kaelbling1996reinforcement}
\citation{strehl2006pac}
\citation{brafman2002r}
\citation{kearns2002near}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Classification/Types}{7}}
\newlabel{types}{{2.3.3}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Model-free}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Model-based}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Q-Learning}{8}}
\@writefile{toc}{\contentsline {subsubsection}{What is Q-Learning?}{8}}
\newlabel{q_function}{{2.1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Q-Learning Algorithm}{9}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Q-Learning Algorithm}}{9}}
\newlabel{q_learning}{{1}{9}}
\@writefile{toc}{\contentsline {subsubsection}{The need of learning Q values.}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An example gridlworld with stochastic actions}}{10}}
\newlabel{fig:gridworld_ex}{{2.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Learning Laser data and space complexity!}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Navigation with perception in the loop}}{10}}
\newlabel{fig:gp_laser}{{2.3}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Introduction to GPs}{11}}
\newlabel{introToGPs}{{2.4}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Gaussian Processes for Machine Learning}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Use of GPs in Q learning}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}The GP-Q Algorithm}{12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{gpq}{{3}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}What is "Perception in the loop"?}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Laser data and feedback}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}How is it useful?}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Simulator setup and Software}{12}}
\citation{cutler2014reinforcement}
\citation{taylor2007transfer}
\citation{li2011knows}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}The GP-MFRL Algorithm}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{gpMfrl}{{4}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Multi-Fidelity Reinforcement Learning}{13}}
\citation{Dames2015}
\citation{rasmussen2003gaussian}
\citation{deisenroth2010efficient}
\citation{engel2005reinforcement}
\citation{puterman2014markov}
\citation{brafman2002r,kearns2002near}
\citation{sutton1998reinforcement}
\citation{strehl2006pac}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces MFRL framework: First simulator captures only gridworld movements of a point robot while second simulator has more fidelity using a physics simulator. Control can switch back and forth between simulators and real environment which is essentially the third simulator in the multi-fidelity simulator chain.}}{14}}
\newlabel{fig:mfrl_architecture}{{4.1}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Learning Transition Dynamics as a GP}{14}}
\citation{rasmussen2003gaussian}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Overview of the GP-MFRL algorithm }}{15}}
\newlabel{fig:gp_mfrl_system}{{4.2}{15}}
\newlabel{gaussian_mean}{{4.1}{15}}
\newlabel{gaussian_variance}{{4.2}{15}}
\citation{abbeel2006using,taylor2007transfer}
\citation{cutler2014reinforcement}
\citation{sutton1998reinforcement}
\newlabel{kernel_def}{{4.3}{16}}
\newlabel{velocity_def}{{4.4}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}GP-MFRL Algorithm}{16}}
\citation{koenig2004design}
\citation{quigley2009ros}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Simulation Results}{18}}
\newlabel{gp-mfrl-sim}{{4.4}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The environment setup for a multi-fidelity simulator chain. The simple gridworld environment has two wall obstacles whereas the gazebo environment has four wall obstacles as shown.}}{18}}
\newlabel{fig:gp_mfrl_setup}{{4.3}{18}}
\newlabel{lt}{{4}{19}}
\newlabel{forins}{{8}{19}}
\newlabel{forins1}{{17}{19}}
\newlabel{forins2}{{20}{19}}
\newlabel{forins3}{{22}{19}}
\newlabel{forins4}{{30}{19}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces GP-MFRL Algorithm}}{19}}
\newlabel{GP-MFRL}{{2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces  The figure represents the samples collected in each level of simulator for a $21 \times 21$ grid in a simple grid-world and Gazebo environments. $\Psi $ and $\psi $ were kept 0.4 and $0.1$}}{20}}
\newlabel{fig:epoch_samples}{{4.4}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after transition dynamics initialization and after algorithm has converged}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {After Initial Training}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After Convergence}}}{20}}
\newlabel{fig:heatmap1}{{4.5}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Representative simulations}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after transition dynamics initialization and after algorithm has converged}}{21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {After Initial Training}}}{21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After Convergence}}}{21}}
\newlabel{fig:heatmap2}{{4.6}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after the algorithm has converged. Walls A and B are only present in the grid-world simulator, whereas all four walls are present in the Gazebo simulator.}}{21}}
\newlabel{fig:heatmap_four_walls_complex}{{4.7}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Effect of fidelity on the number of samples.}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces As we make first simulator more inaccurate by adding noise, the agent tends to gather more samples in second simulator }}{22}}
\newlabel{fig:gp_mfrl_samples}{{4.8}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Ratio of samples gathered in the second simulator to the total samples gathered increases with inaccuracy in the first simulator. The reference line depicts the average number of samples gathered over $10$ runs when only Gazebo simulator was present.}}{22}}
\newlabel{fig:gp_mfrl_ratio}{{4.9}{22}}
\citation{cutler2014reinforcement}
\citation{cutler2014reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Effect of the confidence parameters.}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Ratio of samples gathered in second simulator vs. total samples gathered as we change the threshold or confidence parameters of the two simulators.}}{23}}
\newlabel{fig:threshold}{{4.10}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Comparison with R-max MFRL}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces  Discounted return in the start state Vs. the number of samples collected in the highest fidelity simulator.}}{24}}
\newlabel{fig:value_function}{{4.11}{24}}
\bibdata{refs}
\bibstyle{plain}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Bridge Inspection}{25}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
