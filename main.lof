\select@language {english}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Quadrotor navigating through a confined space without GPS signal. Images collected during field trial reported in Chapter \ref {bridge_chapter}.}}{2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Conventional inspection units for Bridge Inspection: (left, Bridge Inspection platform with a truck crane.), A-30 Hi-Rail Under Bridge Units (right, N.E. Bridge Contractors Inc.)}}{6}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Truck Crane}}}{6}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Under Bridge Inspection unit}}}{6}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Bridgeriggers' Aspen A-75 under-bridge inspection crane overturns on Sakonnet River Bridge in Rhode Island, August 30, 2016. }}{6}
\contentsline {figure}{\numberline {2.3}{\ignorespaces The UAV used for the experiments.}}{8}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Flea3 2.0 MP Color USB3 Vision. (e2v EV76C5706F)}}{8}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Intel NUC NUC5I7RYH}}{9}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Pixhawk Flight Controller}}{9}
\contentsline {figure}{\numberline {2.7}{\ignorespaces LIDAR-Lite v$3$}}{10}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Ublox Neo-M$8$N GPS}}{11}
\contentsline {figure}{\numberline {2.9}{\ignorespaces E800 Tuned Propulsion System by DJI}}{11}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Camera view and the ROS-bag record running nodes on a Linux machine.}}{12}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Indoor flight of the UAV visually inspecting the structure.}}{12}
\contentsline {figure}{\numberline {2.12}{\ignorespaces Pictures captured by the on-board camera at VTTI smart road bridge.}}{13}
\contentsline {figure}{\numberline {2.13}{\ignorespaces Outdoor flight of the UAV visually inspecting the structure at VTTI smart road bridge.}}{14}
\contentsline {figure}{\numberline {2.14}{\ignorespaces Outdoor flight of the UAV visually inspecting the structure at the Coleman Memorial Bridge.}}{14}
\contentsline {figure}{\numberline {2.15}{\ignorespaces Images captured by the on-board camera at the Coleman Memorial Bridge.}}{14}
\contentsline {figure}{\numberline {2.16}{\ignorespaces The 3D scan of the Coleman Memorial Bridge obtained from the collected images.}}{15}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A Simplified Reinforcement Learning Model}}{20}
\contentsline {figure}{\numberline {3.2}{\ignorespaces An example gridlworld with stochastic actions}}{24}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Navigation with perception in the loop}}{25}
\contentsline {figure}{\numberline {3.4}{\ignorespaces To find a function that is consistent over the observed data}}{25}
\contentsline {figure}{\numberline {3.5}{\ignorespaces A naive representation of deep Q-network}}{28}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces A UAV equipped with a laser sensor.}}{30}
\contentsline {figure}{\numberline {4.2}{\ignorespaces A python based simulator for obstacle avoidance.}}{34}
\contentsline {figure}{\numberline {4.3}{\ignorespaces A UAV equipped with laser sensor in Gazebo Robot Simulator.}}{35}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Simulator Environments.}}{36}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Environment 1}}}{36}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Environment 2}}}{36}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Environment 3}}}{36}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Environment 4}}}{36}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Reward obtained over time for environments shown in Figure \ref {fig:gpq_sim}. One \textit {epoch} is a set of $200$ actions.}}{37}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Reward collected in Environment 1}}}{37}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Reward collected in Environment 2}}}{37}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Reward collected in Environment 3}}}{37}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Reward collected in Environment 4}}}{37}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Gazebo Robot Simulator environments and the reward collected over time. One \textit {epoch} is a set of $20$ actions.}}{38}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Gazebo environment 1}}}{38}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Reward collected in Gazebo environment 1}}}{38}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Gazebo environment 2}}}{38}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Reward collected in Gazebo environment 2}}}{38}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces MFRL framework: First simulator captures only gridworld movements of a point robot while second simulator has more fidelity using a physics simulator. Control can switch back and forth between simulators and real environment which is essentially the third simulator in the multi-fidelity simulator chain.}}{41}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Overview of the GP-MFRL algorithm. }}{42}
\contentsline {figure}{\numberline {5.3}{\ignorespaces The environment setup for a multi-fidelity simulator chain. The simple gridworld environment has two wall obstacles whereas the gazebo environment has four wall obstacles as shown.}}{45}
\contentsline {figure}{\numberline {5.4}{\ignorespaces The figure represents the samples collected in each level of simulator for a $21 \times 21$ grid in a simple grid-world and Gazebo environments. $\Psi $ and $\psi $ were kept 0.4 and $0.1$.}}{47}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after transition dynamics initialization and after algorithm has converged.}}{47}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {After Initial Training}}}{47}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {After Convergence}}}{47}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after transition dynamics initialization and after algorithm has converged}}{48}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {After Initial Training}}}{48}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {After Convergence}}}{48}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Variance plot for $21 \times 21$ multi-fidelity environment after the algorithm has converged. Walls A and B are only present in the grid-world simulator, whereas all four walls are present in the Gazebo simulator.}}{48}
\contentsline {figure}{\numberline {5.8}{\ignorespaces As we make first simulator more inaccurate by adding noise, the agent tends to gather more samples in second simulator. }}{49}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Ratio of samples gathered in the second simulator to the total samples gathered increases with inaccuracy in the first simulator. The reference line depicts the average number of samples gathered over $10$ runs when only Gazebo simulator was present.}}{49}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Ratio of samples gathered in second simulator vs. total samples gathered as we change the threshold or confidence parameters of the two simulators.}}{50}
\contentsline {figure}{\numberline {5.11}{\ignorespaces Discounted return in the start state Vs. the number of samples collected in the highest fidelity simulator.}}{51}
\addvspace {10\p@ }
