\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Motivation and applications to general scenarios}{2}
\contentsline {subsection}{\numberline {1.1.1}Challenges}{2}
\contentsline {section}{\numberline {1.2}Contributions}{3}
\contentsline {section}{\numberline {1.3}Organization of the Thesis}{4}
\contentsline {chapter}{\numberline {2}Background}{5}
\contentsline {section}{\numberline {2.1}Sequential Decision Making}{5}
\contentsline {subsection}{\numberline {2.1.1}Approaches to solve sequential decision making problems}{5}
\contentsline {subsection}{\numberline {2.1.2}Rewards, and how to assign them?}{6}
\contentsline {section}{\numberline {2.2}Markov Decision Processes}{6}
\contentsline {subsection}{\numberline {2.2.1}Policies}{7}
\contentsline {subsection}{\numberline {2.2.2}Value function and Bellman equation}{7}
\contentsline {subsection}{\numberline {2.2.3}Solving MDPs}{8}
\contentsline {section}{\numberline {2.3}Reinforcement Learning (RL)}{9}
\contentsline {subsection}{\numberline {2.3.1}Classification/Types}{10}
\contentsline {subsubsection}{Model-free}{10}
\contentsline {subsubsection}{Model-based}{10}
\contentsline {subsection}{\numberline {2.3.2}Online vs off-line learning}{11}
\contentsline {subsection}{\numberline {2.3.3}Exploration vs. Exploitation}{11}
\contentsline {subsection}{\numberline {2.3.4}Q-Learning}{12}
\contentsline {subsubsection}{Example}{13}
\contentsline {section}{\numberline {2.4}Introduction to Gaussian Processes (GP)}{14}
\contentsline {subsubsection}{Kernel Functions}{15}
\contentsline {subsection}{\numberline {2.4.1}Gaussian Process Regression}{16}
\contentsline {section}{\numberline {2.5}Deep Reinforcement Learning}{17}
\contentsline {chapter}{\numberline {3}The End-to-End GPQ Algorithm}{19}
\contentsline {section}{\numberline {3.1}Perception in the loop}{19}
\contentsline {section}{\numberline {3.2}The Simulator setup}{20}
\contentsline {subsection}{\numberline {3.2.1}Laser data and $Q$-Learning}{21}
\contentsline {section}{\numberline {3.3}End-to-End GPQ Algorithm}{22}
\contentsline {section}{\numberline {3.4}Simulations and Results}{22}
\contentsline {subsection}{\numberline {3.4.1}Reward as a function of time}{24}
\contentsline {chapter}{\numberline {4}The GP-MFRL Algorithm}{27}
\contentsline {section}{\numberline {4.1}Multi-Fidelity Reinforcement Learning}{27}
\contentsline {section}{\numberline {4.2}Learning Transition Dynamics as a GP}{28}
\contentsline {section}{\numberline {4.3}GP-MFRL Algorithm}{30}
\contentsline {section}{\numberline {4.4}Simulation Results}{32}
\contentsline {subsection}{\numberline {4.4.1}Representative simulations}{34}
\contentsline {subsection}{\numberline {4.4.2}Effect of fidelity on the number of samples.}{35}
\contentsline {subsection}{\numberline {4.4.3}Effect of the confidence parameters.}{37}
\contentsline {subsection}{\numberline {4.4.4}Comparison with R-max MFRL}{37}
\contentsline {section}{\numberline {4.5}Conclusion}{37}
\contentsline {chapter}{\numberline {5}Bridge Inspection}{39}
\contentsline {section}{\numberline {5.1}Background}{39}
\contentsline {subsection}{\numberline {5.1.1}Conventional Inspection of Bridges}{39}
\contentsline {subsection}{\numberline {5.1.2}UAVs in Bridge Inspection}{41}
\contentsline {section}{\numberline {5.2}Present Work}{41}
\contentsline {subsection}{\numberline {5.2.1}The System Setup}{42}
\contentsline {subsection}{\numberline {5.2.2}Camera Software and ROS nodes}{46}
\contentsline {chapter}{\numberline {6}Conclusion and Future Research}{48}
