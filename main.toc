\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Motivation and applications to general scenarios}{2}
\contentsline {subsection}{\numberline {1.1.1}Challenges}{2}
\contentsline {section}{\numberline {1.2}Contributions}{3}
\contentsline {section}{\numberline {1.3}Organization of the Thesis}{3}
\contentsline {chapter}{\numberline {2}Motivating Application: Bridge Inspection}{5}
\contentsline {section}{\numberline {2.1}Background}{5}
\contentsline {subsection}{\numberline {2.1.1}Conventional Inspection of Bridges}{5}
\contentsline {section}{\numberline {2.2}Preliminary Experiments}{7}
\contentsline {subsection}{\numberline {2.2.1}System Setup}{8}
\contentsline {subsection}{\numberline {2.2.2}Camera Software and ROS nodes}{11}
\contentsline {chapter}{\numberline {3}Background}{16}
\contentsline {section}{\numberline {3.1}Sequential Decision Making}{16}
\contentsline {subsection}{\numberline {3.1.1}Approaches to solve sequential decision making problems}{16}
\contentsline {subsection}{\numberline {3.1.2}Rewards, and how to assign them?}{17}
\contentsline {section}{\numberline {3.2}Markov Decision Processes}{17}
\contentsline {subsection}{\numberline {3.2.1}Policies}{18}
\contentsline {subsection}{\numberline {3.2.2}Value function and Bellman equation}{18}
\contentsline {subsection}{\numberline {3.2.3}Solving MDPs}{19}
\contentsline {section}{\numberline {3.3}Reinforcement Learning (RL)}{20}
\contentsline {subsection}{\numberline {3.3.1}Classification/Types}{21}
\contentsline {subsubsection}{Model-free}{21}
\contentsline {subsubsection}{Model-based}{21}
\contentsline {subsection}{\numberline {3.3.2}Online vs off-line learning}{22}
\contentsline {subsection}{\numberline {3.3.3}Exploration vs. Exploitation}{22}
\contentsline {subsection}{\numberline {3.3.4}Q-Learning}{23}
\contentsline {subsubsection}{Example}{24}
\contentsline {section}{\numberline {3.4}Introduction to Gaussian Processes (GP)}{25}
\contentsline {subsubsection}{Kernel Functions}{26}
\contentsline {subsection}{\numberline {3.4.1}Gaussian Process Regression}{27}
\contentsline {section}{\numberline {3.5}Deep Reinforcement Learning}{28}
\contentsline {chapter}{\numberline {4}The End-to-End GPQ Algorithm}{30}
\contentsline {section}{\numberline {4.1}Laser data and $Q$-Learning}{31}
\contentsline {section}{\numberline {4.2}The Simulator setup}{34}
\contentsline {section}{\numberline {4.3}Simulations and Results}{35}
\contentsline {subsection}{\numberline {4.3.1}Simulation Results}{35}
\contentsline {chapter}{\numberline {5}The GP-MFRL Algorithm}{40}
\contentsline {section}{\numberline {5.1}Multi-Fidelity Reinforcement Learning}{40}
\contentsline {section}{\numberline {5.2}Learning Transition Dynamics as a GP}{41}
\contentsline {section}{\numberline {5.3}GP-MFRL Algorithm}{43}
\contentsline {section}{\numberline {5.4}Simulation Results}{45}
\contentsline {subsection}{\numberline {5.4.1}Representative simulations}{47}
\contentsline {subsection}{\numberline {5.4.2}Effect of fidelity on the number of samples.}{48}
\contentsline {subsection}{\numberline {5.4.3}Effect of the confidence parameters.}{50}
\contentsline {subsection}{\numberline {5.4.4}Comparison with R-max MFRL}{50}
\contentsline {section}{\numberline {5.5}Conclusion}{50}
\contentsline {chapter}{\numberline {6}Conclusion and Future Research}{52}
\contentsline {chapter}{Bibliography}{53}
